:-  :~
  [%owner '~ponnys-podfer']
  [%comments '.y']
  [%type 'fora']
  [%last-modified '~2018.2.25..02.28.04..7050']
  [%date-created '~2018.2.25..02.28.04..7050']
  [%name 'Clay: Longer Term Thoughts']
    ==
;>

I've mentioned some things I want to see in `clay` off-hand in `/urbit-meta`. Here's a longer form take on what were short asides in chat.

## File properties

Where previous file-systems only stored untyped octet streams, Clay is a typed
filesystem. Urbit already has a system to specify code which parses and diffs
files. This should be extended in some way to extracting metadata from a file
and making it indexable by the filesystem.

I previously wrote a [background article about BeOS][beos], but to recap: when
writing a file, the Be Filesystem allowed an additional set of mime typed
key/value pairs to be written alongside the file. Several BeOS specific types
were empty files and used the key/value store as the main data storage method,
such as their address book program. In Urbit, we probably want to implement
arms which extract data from files instead, such as address book data going
into vCard files with a vCard mark which extracts key/value data.

[beos]: https://fora.urbit.org/posts/~2017.9.16..06.19.33..72a4~/

(We may also want to allow users to specify their own metadata on top of this,
but it's not a sure thing and would complicate interfaces.)

In BeOS, the filesystem was not just a traditional filesystem, it also
maintained indexes of properties. Here, the corresponding Urbit design is
unclear: I can see arguments for indexes and query support going directly into
clay, into a separate indexing vane, and into a system provided gall app.

I'm pretty sure the Urbit community has hot takes about which query language to
use. (I've seen [miniKanren][mk] discussed multiple times
on /urbit-meta.) I'm not particular as long as there's a way to make fairly
complex queries on the metadata.

[mk]: http://minikanren.org/

## Harder File Types

Right now, clay only handles simple file types, mainly simple text. Clay will
need to be able to deal with harder cases.

Let's say we have a complete email message mark, which understands all of the
major RFCs about mail, but especially RFC 1341 which defines the multipart mail
format. Email started off as a text format which organically grew support for
multiple binary data streams with some metadata about the file types. MIME,
after all, stands for Multipurpose Internet Mail Extensions.

So a friend sends you an email with three photos attached. Underneath, this is
one email message that (usually) has five files: the `text/plain` version of
the email, the `text/html` version of the email, and the three photos
attached. Usually, one would save the photos, making a duplicate copy of them
on the filesystem.

Urbit can do better. This email is actually a directory with five files in
it. Those five files should all separately go into the underlying
blobstore. Those attachments should be separately addressable in clay, under a
path such as `/=home=/email/msg-id/photo-1.jpg`. This prevents duplicate data as well as making the attachments usable anywhere else in Urbit. A
representation of the email headers should go into a sixth blob, and the entire
message should be castable to &mime to return the original message (important
for checking the DKIM signatures). Since the various Content-Transfer-Encodings
are deterministic, the entire message types can be recreated from the blobs put
in the blob store.

There are a lot of file types like this, where a file is actually a
concatenation of other files. There are obvious ones such as `.iso` disc images
to less obvious ones like `.mp3` files with lyrics and cover art, which is
often large enough that it would be nice to deduplicate across the individual
files.

Which brings us to the much harder case: what happens when you drop a `.zip`
file into clay? A zip file should theoretically be traversable as a series of
directories and the `/=home=/blah.zip/internal-dir/file.jpg` should be
addressable as a clay path. However, it's not clear what happens with the blob
store. There are a lot of different zip encoders which produce compatible, but
different results. You can not just store the uncompressed files from the
`.zip` file in the blob store and expect to be able to reconstruct a byte
identical `.zip` file. This seems like a surmountable problem, but a lot of
thought on edge cases needs to happen.

## The Clay Tracker

Clay is a revision controlled, typed filesystem. What does an equivalent of the
Explorer/Finder/Tracker for it look like? How do we FTFF for Urbit?

- Since all data is typed, the Tracker can be the universal file viewer; all
  marks can just mark transform to the &output format.
  
  - This means that the Tracker also becomes a generic document viewer.

  - Does the Tracker take on file editing? Instead of having fully separate
    apps which are separate, does the Tracker consume everything?

- Clay is revision controlled. The interface should have a first class ability
  to display previous revisions of a file. And since patches themselves are
  typed files, they too can be mark transformed to the &output format.
  
  - Who handles merging?
  
  - If the Tracker becomes responsible for file editing, does it have a commit
    building interface?

- How much emphasis on hierarchical browsing do you have? The traditional
  criticism of current filesystems is that people don't actually organize their
  files (ie, Raskin).

- How much emphasis on metadata based searching do you have? Things like
  Windows Explorer or the Mac Finder have a single search box. The BeOS Tracker
  had a much richer search interface over key/value pair metadata.

- How does the Tracker interact with dojo? This is really a more specific
  version of the general question of how apps interact/integrate with the
  Friendly Command Line. Is there always a dojo prompt at the bottom of each
  context which lets you enter commands? Do UI elements in view translate to
  commands? My gut feeling is that the command line should feel cohesive across
  the entire system instead of being a separate mode that you can go to, but
  I'm unsure if there's any agreement on this.

## Asymptotically approaching Xanadu

Xanadu may never be feasible or achievable, but there are good ideas
there. Looking at it from the viewpoint of a Great Artist, the most
unappreciated idea Nelson proposed was links being two-way objects in the
system. Xanadu as a system implemented this by keeping track of whether two
documents referenced the same text, automatically detecting this
"transclusion," and linking the two documents. This worked in Xanadu because
each copy/paste operation would refer to the same underlying text
object. Modern systems do not work that way.

Documents already have a set of outbound links. I think with basic property
indexing primitives, an 80% solution is possible here without completely
indexing the text. If a file mark extracts an array of detected outbound links
as a file property, this would let the Tracker surface incoming links to the
file currently being viewed. Two way links external to the linked files as a
first class concept could also be implemented.

## Deleting Files

Sometimes, you actually need to make sure a piece of data that went into your
Urbit is no longer in your Urbit.

Right now, the definition of a blob is:

```
++  blob                                              ::  fs blob
  $%  {$delta p/lobe q/{p/mark q/lobe} r/page}        ::  delta on q
      {$direct p/lobe q/page}                         ::  immediate
  ==                                                  ::
```

ie, either a delta to produce the blob or the blob itself.

A `[%deleted ~]` entry should be added to the typed union to represent when a
file has been deleted from your Urbit. This lets you replace a `%delta` or a
`%direct` with a `%deleted`.

If needed, also snapshot and clear the event log if you legally need to have no
copy of the data. Viola.

## Blob storage

Binary asset management is important. Revision control systems don't just handle files where data can be easily diffed, they need to store undiffable binary files. The query "what was the contents of \<file> at revision x?" is as important as "what is the difference between revision x and y for \<file>?"

Perforce (and to a lesser extent svn) continue to see widespread use because they handle this case really well. Both use a centralized server which keeps all previous versions of files, while individual checkouts don't contain all of history. git completely falls down in this case. I am on a team that switched from svn to git and store golden images of our systems output, which get updated fairly often. Everyone's git checkouts are in the low tens of gigabytes now.

We can do better. In clay, an initial sync should just fetch the blobs needed
for the current head. A `[%remote (list @p)]` case should be added to the `++blob` union, listing ships which we know have the blob. Clay then doesn't store the full history, but stores references to where it can go for the full history if needed. Requests can block transparently while the `%remote` gets resolved over the network. Finally, once the ship has the data, the `%remote` can be changed to a `%direct` or `%delta`.

Not only are there benefits to 

## Which way do diffs go?

Right now, as mentioned in the [Clay blogpost][blog-clay], clay currently
stores snapshots and throws away patches, but wishes to eventually store the
patches and regenerate old snapshots from the patches applied.

[blog-clay]: https://urbit.org/blog/clay/

But which way should patches be applied? Do we store forwards or backwards
patches?

```
                         patch 1
         <older file> <----------- <latest file>
```

In the above, we store \<latest file> in full, along with an entry for \<older
file> which references the patch and \<latest file>. We'll call this a backwards
patch, because the patch goes backwards in time.

```
                        patch 1
         <first file> ----------> <newer file>
```

In the above, we store \<first file> in full, along with an entry for \<newer
file> which references the patch and \<first file>. We'll call this a forwards
patch.

Backwards patches and forwards patches are good for different use cases.

Let's say we're polling a website which exposes a discussion thread as a json array of posts. Each time we see the thread updated, we create a patch from the latest json to the previous json, and replace the previous json file with a backwards patch. Backwards patching is perfect for this use case because most patches are going to look something like:

`[[%array-pop 1 /json/path] ~]`

The nice thing about a patch like that is that it can be stored once, but then repeatedly applied across different files. If your general case is adding a single piece of content, your general case is a patch that removes the last piece of content added.

The forward patch of the above would be something like:

`[[%array-push [%o {'subject': [%s 'title'], 'post': [%s 'long post contnet']}]] ~]`

This forward patch is larger, and more importantly, not unique. It can only be applied repeatedly if the exact same content is posted.

So when are forward patches superior? Let's say we're writing an image editor and record actions as semantic forward patches:

`[[%resize 400 200] ~]`

But switching that forward patch to a backward patch would result in something like:

`[[%set-pixel-array ...] ~]`

A giant patch that basically replaces the file! But it gets worse. There isn't a way to turn that backwards patch back into the original forwards patch, which destroys the semantic information that `%resize` has in the first place. If you do a merge between a `%resize` and a `%color-correction` patch, these are mergable. Two forward `%set-pixel-array` patches rebuilt from saved backwards patches conflict with each other. Meaning is lost.

I don't know how to resolve this conflict. One idea is to have two interfaces for writing: one which takes a file state like the current unix facing interfaces and stores a calculated backwards patch (since there is no semantic information in the file write), and one which takes a forward patch to apply to a file like the current internal clay interface and stores the semantic forward patch. But this seems like it complicates things as code now needs to deal with patches going in different directions.


